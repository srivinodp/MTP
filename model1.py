# -*- coding: utf-8 -*-
"""TRY1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16hkBwQvvXfT2_Tf-39-TLf3c1HSBFlyr
"""

from PIL import Image
import glob
from torch.utils.data import Dataset
from torchvision import transforms
import matplotlib.pyplot as plt

trans = transforms.ToTensor()
class CustomDataSet(Dataset):
    def __init__(self, main_dir , transform):
        self.transform = transform
        self.X_vid = glob.glob('/media/Data2/srivinod/' + main_dir + '/X/*')
        
    def __len__(self):
        print(len(self.X_vid),' videos')
        return len(self.X_vid)

    def __getitem__(self, idx):
        
        X, Y = [], []
        path = self.X_vid[idx]
        for i in range(1,17):
            X.append(trans(Image.open(path +'/'+str(i)+'.png').convert("RGB")))
            Y.append(trans(Image.open(path.replace('X','Y') +'/'+str(i)+'.png').convert("RGB")))
            
        return [torch.stack(X, 0).permute(1,0, 2, 3), torch.stack(Y, 0).permute(1,0, 2, 3)]

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math
from functools import partial
class GatedConvolution(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation=1, padding=0, bias=False, type='3d'):
        super(GatedConvolution, self).__init__()
        
        self.phi = nn.Sequential(
                    nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, padding=padding, bias=bias),
                    nn.BatchNorm3d(out_channels),
                    nn.ReLU())

        self.gate = nn.Sequential(
                    nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, padding=padding, bias=bias),
                    nn.Sigmoid())
        
    def forward(self, x):
        phi = self.phi(x)
        gate = self.gate(x)
        # print(phi.shape,gate.shape)
        return phi * gate


class GatedUpConvolution(nn.Module):
    def __init__(self, size, in_channels, out_channels, kernel_size, stride, padding, bias, mode='trilinear'):
        super(GatedUpConvolution, self).__init__()
        
        
        self.phi = nn.Sequential(
                    nn.Upsample(size=size, mode=mode),
                    nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),
                    nn.BatchNorm3d(out_channels),
                    nn.LeakyReLU(0.2)
                    )
        self.gate = nn.Sequential(
                    nn.Upsample(size=size, mode=mode),
                    nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),
                    nn.Sigmoid()
                    )
        
    def forward(self, x):
        phi = self.phi(x)
        gate = self.gate(x)
        # print(phi.shape,gate.shape)
        return phi * gate

class ICNetDeepGate(nn.Module):
    def __init__(self):
        super(ICNetDeepGate, self).__init__()
        # self.opt = opt

        ### encoder
        self.ec0 = GatedConvolution(3, 64, kernel_size=(3,3,3), stride=(2,2,2), padding=(1,1,1), bias=False)
        self.ec1 = GatedConvolution(64, 128, kernel_size=(3,3,3), stride=(2,2,2), padding=(1,1,1), bias=False)
        self.ec2 = GatedConvolution(128, 256, kernel_size=(3,3,3), stride=(1,2,2), padding=(0,1,1), bias=False)

        ### temporal convs
        self.tc0 = GatedConvolution(64, 64, kernel_size=(5,3,3), stride=(1,1,1), padding=(0,1,1),bias=False)
        self.tc1 = GatedConvolution(128,128, kernel_size=(3,3,3), stride=(1,1,1), padding=(0,1,1),bias=False)

        ### bottleneck
        self.bt0 = GatedConvolution(256, 256, kernel_size=(1,3,3), stride=(1,1,1), dilation=(1,1,1), padding=(0,1,1), bias=False)
        self.bt1 = GatedConvolution(256, 256, kernel_size=(1,3,3), stride=(1,1,1), dilation=(1,2,2), padding=(0,2,2), bias=False)
        self.bt2 = GatedConvolution(256, 256, kernel_size=(1,3,3), stride=(1,1,1), dilation=(1,4,4), padding=(0,4,4), bias=False)

        ### decoder
        self.dc2 = GatedUpConvolution((2, 32, 32), 256 + 256, 256, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)
        self.dc1 = GatedUpConvolution((4, 64, 64), 256 + 128, 128, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)
        self.dc0 = GatedUpConvolution((16, 128, 128), 128 + 64, 64, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=False)

        self.clip_diff_p = nn.Conv3d(64, 3, kernel_size=(1,3,3), stride=(1,1,1), padding=(0,1,1), bias=True)

        # for m in self.modules():
        #     if isinstance(m, nn.Conv3d):
        #         m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')
        #     elif isinstance(m, nn.BatchNorm3d):
        #         m.weight.data.fill_(1)
        #         m.bias.data.zero_()
    
    def forward(self, x):

        ### encoder
        e0 = self.ec0(x)
        e1 = self.ec1(e0)
        # print(x.shape, e0.shape, e1.shape)
        e2 = self.ec2(e1)

        ### temporal convs
        t0 = self.tc0(e0)
        t1 = self.tc1(e1)

        ### bottleneck
        bt0 = self.bt0(e2)  
        bt1 = self.bt1(bt0) 
        bt2 = self.bt2(bt1) 

        # pdb.set_trace()
        ### decoder
        # print('input - ',x.shape)
        # print('e0,e1,e2 - ',e0.shape,e1.shape,e2.shape)
        # print('t0,t1 - ',t0.shape,t1.shape)
        # print('bt0,bt1,bt2 - ',bt0.shape,bt1.shape,bt2.shape)


        d2 = self.dc2(torch.cat((bt2, e2),1)) # 16 --> 32
        
        # print('d2 - ',d2.shape)
        
        d1 = self.dc1(torch.cat((d2, t1), 1)) # 32 --> 64
        
        # print('d1 - ',d1.shape)
        
        d0 = self.dc0(torch.cat((d1, t0), 1))
        
        # print('d0 -',d0.shape)
        
        clip_pred = x[:,:,4,:,:].unsqueeze(2) + self.clip_diff_p(d0) # 64 --> 3
        return torch.sigmoid(clip_pred)

import torch
from torch.autograd import Variable


def train_model(epochs):
    best_loss = 1000;
    for e in range(epochs):
        tot_loss = 0
        print('Batch :',end = ' ')
        for idx,l in enumerate(train_loader):
            X,Y = l[0].cuda(), l[1].cuda()
            o = model(X)
            loss = nn.L1Loss()(o,Y)
            tot_loss += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if idx%50 == 0:
                print(loss.item(), end = ',')
        print()
        print('Epoch',e,' Loss: ',tot_loss/len(train_loader))#total batches
        loss_curve.append(tot_loss/len(train_loader))
        if(best_loss>tot_loss/len(train_loader)):
            best_loss = tot_loss/len(train_loader)
            torch.save(model.state_dict(), '/media/Data2/srivinod/3dcnn')
            # plt.plot(loss_curve)
            # plt.ylabel('loss')
            # plt.xlabel('epochs')
            # plt.plot()

import warnings
warnings.filterwarnings("ignore")

from torch import optim

bsize = 5
my_dataset = CustomDataSet('train4', transform=None)
train_loader = torch.utils.data.DataLoader(my_dataset , batch_size = bsize, shuffle=False, num_workers=4)

model = ICNetDeepGate().cuda()
optimizer = optim.Adam(model.parameters(), lr = 0.01, betas=(0.9, 0.999))
#model.load_state_dict(torch.load('/media/Data2/srivinod/FEB_128'))

loss_curve = []
train_model(300)


